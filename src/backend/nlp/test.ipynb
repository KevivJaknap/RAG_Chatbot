{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoModelForQuestionAnswering\n",
    "import faiss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'C:\\Users\\vivek_pankaj\\Desktop\\DAP\\dap_app_v1.0\\src\\files'\n",
    "file_paths = [os.path.join(base_path, file) for file in os.listdir(base_path) if file.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAISSModel:\n",
    "    def __init__(self, model_name, file_paths):\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.data_process(file_paths)\n",
    "        self.build_index(self.split_texts)\n",
    "\n",
    "    \n",
    "    def splitter(self, text):\n",
    "        character_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        sentence_splitter = SentenceTransformersTokenTextSplitter(\n",
    "            tokens_per_chunk=256,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "\n",
    "        character_split_texts = character_splitter.split_text(text)\n",
    "        token_split_texts = []\n",
    "        for character_split_text in character_split_texts:\n",
    "            token_split_texts.extend(sentence_splitter.split_text(character_split_text))\n",
    "        \n",
    "        self.split_texts = token_split_texts\n",
    "    \n",
    "    def data_process(self, file_paths):\n",
    "        total_text = '\\n\\n'.join([self.extract_pdf(file_path) for file_path in file_paths])\n",
    "        self.splitter(total_text)\n",
    "\n",
    "    def build_index(self, split_texts):\n",
    "        embeddings = self.generate_embeddings(split_texts)\n",
    "        self.index = faiss.IndexFlatIP(self.model.config.hidden_size)\n",
    "        self.index.add(embeddings)\n",
    "    \n",
    "    def get_best_match(self, query):\n",
    "        query_embeddings = self.generate_embeddings([query])\n",
    "        D, I = self.index.search(query_embeddings, 1)\n",
    "        return self.split_texts[I[0][0]]\n",
    "\n",
    "    def get_k_matches(self, query, k):\n",
    "        query_embeddings = self.generate_embeddings([query])\n",
    "        D, I = self.index.search(query_embeddings, k)\n",
    "        return [self.split_texts[i] for i in I[0]]\n",
    "    \n",
    "    def generate_embeddings(self, chunks):\n",
    "        inputs = self.tokenizer(chunks, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy()\n",
    "    \n",
    "    def check_pdf_exists(self, file_path: str) -> bool:\n",
    "        '''\n",
    "        Check if the file exists and is a pdf file\n",
    "        '''\n",
    "        if os.path.isfile(file_path) and file_path.lower().endswith('.pdf'):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def extract_pdf(self, file_path: str) -> str | None:\n",
    "        '''\n",
    "        Extract text from a pdf file\n",
    "        '''\n",
    "        if self.check_pdf_exists(file_path):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf = PdfReader(file)\n",
    "                text = ''\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "                return text\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vivek_pankaj\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "faiss_model = FAISSModel('sentence-transformers/all-MiniLM-L6-v2', file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fiscal year 2023 compared with fiscal year 2022 interest and dividends income increased due to higher yields, offset in part by lower portfolio balances. interest expense decreased due to a decrease in outstanding long - term debt due to debt maturities. net recognized gains on investments', 'june 30, 2023 changes in fair value recorded in other comprehensive income commercial paper level 2 $ 16, 589 $ 0 $ 0 $ 16, 589 $ 12, 231 $ 4, 358 $ 0 certificates of deposit level 2 2, 701 0 0 2, 701 2, 657 44 0', 'in order to manage our costs in a dynamic, competitive environment, in fiscal year 2023 we announced that base salaries of salaried employees would remain at fiscal year 2022 levels. pay increases continue to be available for rewards - eligible hourly and eq uivalent employees. we will continue our practice of investing in stock for all rewards - eligible employees,', 'segment revenue and operating income were as follows during the periods presented : ( in millions ) year ended june 30, 2023 2022 2021 revenue productivity and business processes $ 69, 274 $ 63, 364 $ 53, 915 intelligent cloud 87, 907 74, 965 59, 728 more personal computing 54, 734 59, 941 54, 445', 'basis cash and cash equivalents short - term investments equity investments june 30, 2022 changes in fair value recorded in other comprehensive income commercial paper level 2 $ 2, 500 $ 0 $ 0 $ 2, 500 $ 2, 498 $ 2 $ 0']\n"
     ]
    }
   ],
   "source": [
    "query = 'Did base salaries increase from fiscal year 2022 to fiscal year 2023?'\n",
    "print(faiss_model.get_k_matches(query, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ARLI_API_KEY'] = 'e8ad995f-a8c6-4814-b068-5e65be41c605'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arli_response(response: str) -> list:\n",
    "    k = list(response.split('data: '))\n",
    "    k = filter(lambda x: x != '', k)\n",
    "    arr = []\n",
    "    for obj in k:\n",
    "        try:\n",
    "            arr.append(json.loads(obj))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return ''.join([obj['choices'][0]['text'] for obj in arr])\n",
    "\n",
    "def generate_prompt(user_input: str, system_msg: str) -> str:\n",
    "    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "def arli_pipeline(user_input: str, system_msg: str = \"You are a useful AI assistant\") -> str:\n",
    "    url = \"https://api.arliai.com/v1/completions\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "    \"model\": \"Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"prompt\": generate_prompt(user_input=user_input, system_msg=system_msg),\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f\"Bearer {os.environ['ARLI_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    return process_arli_response(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOGETHER_API_KEY'] = '5ddfdebe1a39c353e040a6e8de725209be9022fea728ff075707488ad8423fef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "def together_rag_pipeline(query: str, context: str) -> str:\n",
    "    client = Together()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the given context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a useful AI assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "  messages=[{\"role\": \"user\", \"content\": \"What are the top 3 things to do in New York?\"}],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New York City is a vibrant and diverse metropolis with countless attractions and activities to suit all interests. Here are three top things to do in New York:\\n\\n1. **Visit the Statue of Liberty and Ellis Island**: Take a ferry to Liberty Island to see the iconic Statue of Liberty up close and learn about its history and significance. You can also visit the Ellis Island Immigration Museum to explore the history of immigration in the United States.\\n\\n2. **Explore the Metropolitan Museum of Art**: The Met, as it's commonly known, is one of the world's largest and most famous museums, with a collection that spans over 5,000 years of human history. From ancient Egyptian artifacts to modern and contemporary art, the Met has something for everyone.\\n\\n3. **Walk across the Brooklyn Bridge**: Take in the stunning views of the Manhattan skyline and the East River by walking across the iconic Brooklyn Bridge. You can also stop at the Brooklyn Bridge Park for a picnic or a stroll along the waterfront.\\n\\nOf course, there are many more things to see and do in New York, but these three activities give you a great taste of the city's history, culture, and natural beauty.\\n\\nAdditional suggestions:\\n- Visit the 9/11 Memorial & Museum\\n- Take a stroll through Central Park\\n- See a Broadway show\\n- Visit the Top of the Rock Observation Deck\\n- Explore the High Line\\n- Visit the American Museum of Natural History\\n- Take a food tour of Chinatown or Little Italy\\n- Visit the Guggenheim Museum\\n\\nThese are just a few of the many amazing experiences that New York City has to offer.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMV2:\n",
    "    def _init__(self):\n",
    "        pass\n",
    "    \n",
    "    def response(self, query: str, context: str) -> str:\n",
    "        prompt = f\"\"\"Context: {context} \\n\\nQuery: {query} Answer:\"\"\"\n",
    "        return arli_pipeline(user_input=prompt, system_msg=\"You are a useful AI assistant which reads the context and answers the query\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the given context, there are 1000 people working in Merilytics.\n"
     ]
    }
   ],
   "source": [
    "context_lst = [\"There are 1000 people working in Merilytics.\", \"Merilytics is a data analytics company.\", \"Merilytics is based in Hyderabad.\"]\n",
    "llm = LLMV2()\n",
    "context = \"\\n\\n\".join(context_lst)\n",
    "print(llm.response(\"How many people work in Merilytics?\", context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merilytics is a data analytics company.\n"
     ]
    }
   ],
   "source": [
    "print(llm.response(\"What type of company is Merilytics?\", context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_transformation(query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Rephrase the question to better structure so that it returns better answers. Return two queries in json format. Do not use abbreviations or acronyms. Do not change the meaning of the question.\n",
    "    At the start of the json, provide <JSON> and at the end provide </JSON>\n",
    "    \\n\\n\\n\n",
    "    Question: {query}\n",
    "    \\n\\n\\n\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "    system_msg = \"You are a useful AI assistant\"\n",
    "    return arli_pipeline(user_input=prompt, system_msg=system_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statement_assumption(query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Generate a hypothetical answer to the following query in one or two sentences.\n",
    "    \\n\\n\n",
    "    Query: {query}\n",
    "    \\n\\n\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    system_msg = \"You are a useful AI assistant\"\n",
    "    return arli_pipeline(user_input=prompt, system_msg=system_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Rephrase the question to better structure so that it returns better answers. Return two queries in json format. Do not use abbreviations or acronyms. Do not change the meaning of the question.\n",
      "    At the start of the json, provide <JSON> and at the end provide </JSON>\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    Question: Did base salaries increase from fiscal year 2022 to fiscal year 2023?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = \"Did base salaries increase from fiscal year 2022 to fiscal year 2023?\"\n",
    "k = query_transformation(query)\n",
    "# print(statement_assumption(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in order to manage our costs in a dynamic, competitive environment, in fiscal year 2023 we announced that base salaries of salaried employees would remain at fiscal year 2022 levels. pay increases continue to be available for rewards - eligible hourly and eq uivalent employees. we will continue our practice of investing in stock for all rewards - eligible employees,', 'fiscal year 2023 compared with fiscal year 2022 interest and dividends income increased due to higher yields, offset in part by lower portfolio balances. interest expense decreased due to a decrease in outstanding long - term debt due to debt maturities. net recognized gains on investments', 'increased to 67. 0 million. • linkedin revenue increased 10 %. • dynamics products and cloud services revenue increased 16 % driven by dynamics 365 growth of 24 %. • server products and cloud services revenue increased 19 % driven by azure and other cloud services growth of 29 %. • windows original equipment manufacturer licensing ( “ windows oem ” ) revenue decreased 25 %.', '• microsoft cloud revenue increased 22 % to $ 111. 6 billion. • office commercial products and cloud services revenue increased 10 % driven by office 365 commercial growth of 13 %. • office consumer products and cloud services revenue increased 2 % and microsoft 365 consumer subscribers increased to 67. 0 million. • linkedin revenue increased 10 %.', 'of 24 %. operating income increased $ 4. 5 billion or 15 %. • gross margin increased $ 5. 8 billion or 12 % driven by growth in office 365 commercial and linkedin, as well as the change in accounting estimate. gross margin percentage increased. excluding the impact of the change in accounting estimate, gross margin pe rcentage increased slightly driven by improvement in office 365']\n"
     ]
    }
   ],
   "source": [
    "query = \"According to our internal data analysis, base salaries increased by an average of 4.5% across various industries and sectors from fiscal year 2022 to fiscal year 2023, driven primarily by inflation adjustments and market rate changes. However, certain high-growth fields such as technology and healthcare saw even more substantial increases, averaging around 7-8% during this period.\"\n",
    "print(faiss_model.get_k_matches(query, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<JSON>\n",
      "\n",
      "{\n",
      "  \"query1\": \"What is the average annual salary for employees as of the end of fiscal year 2022 compared to the same metric for fiscal year 2023?\",\n",
      "  \"query2\": \"Were there any noticeable changes in the frequency or amount of base salary increases given to employees during the transition from fiscal year 2022 to fiscal year 2023?\"\n",
      "}\n",
      "\n",
      "</JSON>\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query_transformation(response: str) -> dict:\n",
    "    start_tag = \"<JSON>\"\n",
    "    end_tag = \"</JSON>\"\n",
    "    response = response.strip()\n",
    "    start = response.find(start_tag)\n",
    "    end = response.find(end_tag)\n",
    "    if start == -1 or end == -1:\n",
    "        return {}\n",
    "    substring = response[start + len(start_tag):end].strip()\n",
    "    return json.loads(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = statement_assumption(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = parse_query_transformation(k)\n",
    "queries['query3'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query1': 'What is the average annual salary for employees as of the end of fiscal year 2022 compared to the same metric for fiscal year 2023?', 'query2': 'Were there any noticeable changes in the frequency or amount of base salary increases given to employees during the transition from fiscal year 2022 to fiscal year 2023?', 'query3': 'According to our hypothetical data, base salaries for most industries increased by an average of 4.5% from fiscal year 2022 to fiscal year 2023, with some sectors experiencing even higher growth rates due to market demand and inflationary pressures. This upward trend was driven primarily by efforts to retain top talent and keep pace with rising living costs.'}\n"
     ]
    }
   ],
   "source": [
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_big_context(queries: list[str]) -> str:\n",
    "    context_lst = []\n",
    "    for query in queries:\n",
    "        context_lst.extend(faiss_model.get_k_matches(query, 5))\n",
    "    \n",
    "    counter = Counter(context_lst)\n",
    "    ret_context = ''\n",
    "    best_n_context = counter.most_common(5)\n",
    "    for context, _ in best_n_context:\n",
    "        ret_context += context + '\\n\\n'\n",
    "    return ret_context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_context = get_big_context(list(queries.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = together_rag_pipeline(query=query, context=big_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, base salaries of salaried employees remained at fiscal year 2022 levels in fiscal year 2023.\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_rag_pipeline(query: str) -> str:\n",
    "    transformed_queries = query_transformation(query)\n",
    "    queries = parse_query_transformation(transformed_queries)\n",
    "    statement_assumption_query = statement_assumption(query)\n",
    "    queries['query3'] = statement_assumption_query\n",
    "    big_context = get_big_context(list(queries.values()))\n",
    "    return together_rag_pipeline(query=query, context=big_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG2:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def query_transformation(self, query):\n",
    "        prompt = f\"\"\"\n",
    "        Rephrase the question to better structure so that it returns better answers. Return two queries in json format. Do not use abbreviations or acronyms. Do not change the meaning of the question.\n",
    "        At the start of the json, provide <JSON> and at the end provide </JSON>\n",
    "        \\n\\n\\n\n",
    "        Question: {query}\n",
    "        \\n\\n\\n\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        system_msg = \"You are a useful AI assistant\"\n",
    "        return self.arli_pipeline(user_input=prompt, system_msg=system_msg)\n",
    "    \n",
    "    def arli_pipeline(self, user_input: str, system_msg: str = \"You are a useful AI assistant\") -> str:\n",
    "        url = \"https://api.arliai.com/v1/completions\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "        \"model\": \"Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"prompt\": self.generate_prompt(user_input=user_input, system_msg=system_msg),\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 40,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stream\": True\n",
    "        })\n",
    "        headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f\"Bearer {os.environ['ARLI_API_KEY']}\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        return self.process_arli_response(response.text)\n",
    "    \n",
    "    def process_arli_response(self, response: str) -> list:\n",
    "        k = list(response.split('data: '))\n",
    "        k = filter(lambda x: x != '', k)\n",
    "        arr = []\n",
    "        for obj in k:\n",
    "            try:\n",
    "                arr.append(json.loads(obj))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        return ''.join([obj['choices'][0]['text'] for obj in arr])\n",
    "    \n",
    "    def parse_query_transformation(self, response: str) -> dict:\n",
    "        start_tag = \"<JSON>\"\n",
    "        end_tag = \"</JSON>\"\n",
    "        response = response.strip()\n",
    "        start = response.find(start_tag)\n",
    "        end = response.find(end_tag)\n",
    "        if start == -1 or end == -1:\n",
    "            return {}\n",
    "        substring = response[start + len(start_tag):end].strip()\n",
    "        return json.loads(substring)\n",
    "    \n",
    "    def statement_assumption(self, query: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Generate a hypothetical answer to the following query in one or two sentences.\n",
    "        \\n\\n\n",
    "        Query: {query}\n",
    "        \\n\\n\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        system_msg = \"You are a useful AI assistant\"\n",
    "        return self.arli_pipeline(user_input=prompt, system_msg=system_msg)\n",
    "    \n",
    "    def get_big_context(self, queries: list[str]) -> str:\n",
    "        context_lst = []\n",
    "        for query in queries:\n",
    "            context_lst.extend(faiss_model.get_k_matches(query, 5))\n",
    "\n",
    "        counter = Counter(context_lst)\n",
    "        ret_context = ''\n",
    "        best_n_context = counter.most_common(5)\n",
    "        for context, _ in best_n_context:\n",
    "            ret_context += context + '\\n\\n'\n",
    "        return ret_context\n",
    "    \n",
    "    def advanced_rag_pipeline(self, query: str) -> str:\n",
    "        transformed_queries = self.query_transformation(query)\n",
    "        queries = self.parse_query_transformation(transformed_queries)\n",
    "        statement_assumption_query = self.statement_assumption(query)\n",
    "        queries['query3'] = statement_assumption_query\n",
    "        big_context = self.get_big_context(list(queries.values()))\n",
    "        return self.together_rag_pipeline(query=query, context=big_context)\n",
    "    \n",
    "    def generate_prompt(self, user_input: str, system_msg: str) -> str:\n",
    "        return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    def together_rag_pipeline(self, query: str, context: str) -> str:\n",
    "        client = Together()\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Answer the following question based on the given context:\n",
    "        {context}\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a useful AI assistant\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    \n",
    "    def response(self, query: str) -> str:\n",
    "        return advanced_rag_pipeline(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Rephrase the question to better structure so that it returns better answers. Return two queries in json format. Do not use abbreviations or acronyms. Do not change the meaning of the question.\n",
      "    At the start of the json, provide <JSON> and at the end provide </JSON>\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    Question: Who are the competitors to Microsoft in middleware services?\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "According to the given context, the competitors to Microsoft in middleware services are Java vendors.\n"
     ]
    }
   ],
   "source": [
    "rag = RAG2()\n",
    "print(rag.response(\"Who are the competitors to Microsoft in middleware services?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
